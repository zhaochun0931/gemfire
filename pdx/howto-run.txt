Portable Data Exchange (PDX) is Apache Geodeâ€™s data serialization protocol for cross-language objects and JSON data. When a PDX-serializable object is serialized for the first time, a PdxType is generated for it. The PdxType represents the data structure of that object and is used to serialize and deserialize it.

PdxTypes can proliferate in the TypeRegistry especially with unstructured JSON data. The structure of a class is the same for every instance of that class. The same is not necessarily true of JSON data. If uniquely-structured JSON data is added to a Region, it generates a PdxType specific to it. If that data is then deleted, its PdxType remains in the TypeRegistry and becomes an unused orphan.













docker network create gf-network

docker run -it -d -v $(pwd)/gf-locator1:/data -e 'ACCEPT_TERMS=y' --rm --name gf-locator --network=gf-network -p 10334:10334 -p 7070:7070 gemfire/gemfire:10.1.3 gfsh start locator --name=locator1

docker run -it -d -v $(pwd)/gf-server1:/data -e 'ACCEPT_TERMS=y' --rm --name gf-server1 --network=gf-network -p 40404:40404 gemfire/gemfire:10.1.3 gfsh start server --name=server1 --locators=gf-locator\[10334\]

docker run -it -d -v $(pwd)/gf-server2:/data -e 'ACCEPT_TERMS=y' --rm --name gf-server2 --network=gf-network -p 40405:40405 gemfire/gemfire:10.1.3 gfsh start server --name=server2 --locators=gf-locator\[10334\]



docker exec -it gf-locator gfsh

connect

list members
















start locator --name=locator1


# below command should be issued before starting any data members

configure pdx --read-serialized=true

configure pdx --read-serialized=true --disk-store=pdx_store



start server --name=server1



create disk-store --name=pdx_store --dir=. --max-oplog-size=10
create disk-store --name=data_store --dir=. --max-oplog-size=10

list disk-stores



gfsh>list disk-stores
Member Name |               Member Id                | Disk Store Name | Disk Store ID
----------- | -------------------------------------- | --------------- | ------------------------------------
server1     | 172.16.204.135(server1:6065)<v1>:56935 | data_store      | 9c3509e6-2bfa-4ddb-9746-5cdf5af3c418
server1     | 172.16.204.135(server1:6065)<v1>:56935 | pdx_store       | a25aa90f-6591-4cea-9a8a-09f0d4779c98

gfsh>






create region --name=exampleRegion1 --type=REPLICATE_PERSISTENT --disk-store=data_store

create region --name=exampleRegion2 --type=REPLICATE_PERSISTENT --disk-store=data_store

create region --name=People --type=REPLICATE_PERSISTENT --disk-store=data_store


query --query='select * from /exampleRegion1'



query --query='select * from /PdxTypes' --member=server1

gfsh>query --query='select * from /PdxTypes' --member=server1
Result : true
Limit  : 100
Rows   : 1

typeId  | className | noDomainClass | hasDeletedField |           fields           |     fieldNames      | typeNum | undeletedFieldCount |        sortedFields        |    sortedIdentityFields    | variableLengthFieldCount | fieldCount | dsid
------- | --------- | ------------- | --------------- | -------------------------- | ------------------- | ------- | ------------------- | -------------------------- | -------------------------- | ------------------------ | ---------- | ----
7868271 | "Person"  | false         | false           | [{"fieldName":"id","fiel.. | ["id","name","age"] | 7868271 | 0                   | [{"fieldName":"age","fie.. | [{"fieldName":"age","fie.. | 1                        | 3          | 0

gfsh>
gfsh>
gfsh>
gfsh>list regions
List of regions
---------------
exampleRegion1

gfsh>





gfsh>query --query='select * from /PdxTypes' --member=server1
Result : true
Limit  : 100
Rows   : 2

 typeId  | className | noDomainClass | hasDeletedField |           fields           | typeNum  | undeletedFieldCount |        sortedFields        |     fieldNames      |    sortedIdentityFields    | variableLengthFieldCount | fieldCount | dsid
-------- | --------- | ------------- | --------------- | -------------------------- | -------- | ------------------- | -------------------------- | ------------------- | -------------------------- | ------------------------ | ---------- | ----
12117126 | "Student" | false         | false           | [{"fieldName":"id","fiel.. | 12117126 | 0                   | [{"fieldName":"age","fie.. | ["id","name","age"] | [{"fieldName":"age","fie.. | 1                        | 3          | 0
6994697  | "Person"  | false         | false           | [{"fieldName":"id","fiel.. | 6994697  | 0                   | [{"fieldName":"age","fie.. | ["id","name","age"] | [{"fieldName":"age","fie.. | 1                        | 3          | 0

gfsh>






[info 2025/10/17 13:14:27.407 UTC server1 <main> tid=0x1] Initializing region PdxTypes

[info 2025/10/17 13:14:27.410 UTC server1 <main> tid=0x1] Region /PdxTypes recovered from the local disk. Old persistent ID: /172.16.204.135:/root/pdx/server1/. created at timestamp 1760705959651 version 0 diskStoreId 1819bc775ab045d6-86a923be2d87725b name server1, new persistent ID 172.16.204.135/172.16.204.135:/root/pdx/server1/. created at timestamp 1760706867406 version 0 diskStoreId 1819bc775ab045d6-86a923be2d87725b name server1

[info 2025/10/17 13:14:27.412 UTC server1 <main> tid=0x1] Initialization of region PdxTypes completed







[info 2025/10/17 13:03:38.647 UTC server1 <ServerConnection on port 40404 Thread 2> tid=0x4e] Adding new type: PdxType[dsid=0, typenum=7868271
        name=Person
        fields=[
        id:String:0:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=-1
        name:String:1:1:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=1
        age:int:2:1:idx0(relativeOffset)=-4:idx1(vlfOffsetIndex)=-1]]

[info 2025/10/17 13:03:38.669 UTC server1 <ServerConnection on port 40404 Thread 2> tid=0x4e] Caching PdxType[dsid=0, typenum=7868271
        name=Person
        fields=[
        id:String:0:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=-1
        name:String:1:1:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=1
        age:int:2:1:idx0(relativeOffset)=-4:idx1(vlfOffsetIndex)=-1]]














gfsh>destroy disk-store --name=pdx_store
Member  | Status | Message
------- | ------ | -----------------------------------------------------------
server1 | ERROR  | Disk store is currently in use by these regions [/PdxTypes]

gfsh>
















shutdown --include-locators=yes


gfsh>describe offline-disk-store --name=data_store --disk-dirs=/root/pdx/server1/ --pdx=true
Regions in the disk store:
  /People: -lru=none -concurrencyLevel=16 -initialCapacity=16 -loadFactor=0.75 -offHeap=false -compressor=none -statisticsEnabled=false
PDX Types:
PDX Enums:


gfsh>
gfsh>
gfsh>
gfsh>describe offline-disk-store --name=pdx_store --disk-dirs=/root/pdx/server1/ --pdx=true
Regions in the disk store:
  /PdxTypes: -lru=none -concurrencyLevel=16 -initialCapacity=16 -loadFactor=0.75 -offHeap=false -compressor=none -statisticsEnabled=false
PDX Types:
  App$Person: id=8334571
    String name;
    int age;
PDX Enums:


gfsh>









export data --member=server1 --region=/exampleRegion1 --dir=/tmp --parallel=


gfsh>export data --member=server1 --region=/exampleRegion1 --dir=/tmp --parallel=
Member  | Status | Message
------- | ------ | ----------------------------------------------------------------------------------------------------
server1 | OK     | Data successfully exported from region : /exampleRegion1 to file : /tmp/exampleRegion1.gfd on host..

gfsh>


# delete the pdx_store will cause the following error:



gfsh>query --query='select * from /exampleRegion1'
Result  : false
Message : Unknown pdx type=12117126

gfsh>




[info 2025/10/17 14:26:29.292 UTC server1 <Function Execution Processor2> tid=0x48] Exception occurred:
java.lang.IllegalStateException: Unknown pdx type=12117126
	at org.apache.geode.internal.InternalDataSerializer.readPdxSerializable(InternalDataSerializer.java:3125)
	at org.apache.geode.internal.InternalDataSerializer.basicReadObject(InternalDataSerializer.java:2874)
	at org.apache.geode.DataSerializer.readObject(DataSerializer.java:3189)
	at org.apache.geode.internal.util.BlobHelper.deserializeBlob(BlobHelper.java:110)
	at org.apache.geode.internal.cache.EntryEventImpl.deserialize(EntryEventImpl.java:2067)
	at org.apache.geode.internal.cache.EntryEventImpl.deserialize(EntryEventImpl.java:2059)
	at org.apache.geode.internal.cache.PreferBytesCachedDeserializable.getDeserializedValue(PreferBytesCachedDeserializable.java:79)
	at org.apache.geode.internal.cache.LocalRegion.getDeserialized(LocalRegion.java:1305)
	at org.apache.geode.internal.cache.NonTXEntry.getValue(NonTXEntry.java:95)
	at org.apache.geode.internal.cache.EntriesSet$EntriesIterator.moveNext(EntriesSet.java:189)
	at org.apache.geode.internal.cache.EntriesSet$EntriesIterator.<init>(EntriesSet.java:126)
	at org.apache.geode.internal.cache.EntriesSet.iterator(EntriesSet.java:92)
	at org.apache.geode.cache.query.internal.ResultsCollectionWrapper.iterator(ResultsCollectionWrapper.java:210)
	at org.apache.geode.cache.query.internal.QRegion.iterator(QRegion.java:290)
	at org.apache.geode.cache.query.internal.CompiledSelect.doNestedIterations(CompiledSelect.java:839)
	at org.apache.geode.cache.query.internal.CompiledSelect.doIterationEvaluate(CompiledSelect.java:706)
	at org.apache.geode.cache.query.internal.CompiledSelect.evaluate(CompiledSelect.java:423)
	at org.apache.geode.cache.query.internal.CompiledSelect.evaluate(CompiledSelect.java:53)
	at org.apache.geode.cache.query.internal.DefaultQuery.executeUsingContext(DefaultQuery.java:460)
	at org.apache.geode.cache.query.internal.DefaultQuery.execute(DefaultQuery.java:360)
	at org.apache.geode.cache.query.internal.DefaultQuery.execute(DefaultQuery.java:296)
	at org.apache.geode.management.internal.cli.functions.DataCommandFunction.select(DataCommandFunction.java:218)
	at org.apache.geode.management.internal.cli.functions.DataCommandFunction.select(DataCommandFunction.java:178)
	at org.apache.geode.management.internal.cli.functions.DataCommandFunction.execute(DataCommandFunction.java:127)
	at org.apache.geode.internal.cache.MemberFunctionStreamingMessage.process(MemberFunctionStreamingMessage.java:205)
	at org.apache.geode.distributed.internal.DistributionMessage.scheduleAction(DistributionMessage.java:349)
	at org.apache.geode.distributed.internal.DistributionMessage$1.run(DistributionMessage.java:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.geode.distributed.internal.ClusterOperationExecutors.runUntilShutdown(ClusterOperationExecutors.java:517)
	at org.apache.geode.distributed.internal.ClusterOperationExecutors.doFunctionExecutionThread(ClusterOperationExecutors.java:421)
	at org.apache.geode.logging.internal.executors.LoggingThreadFactory.lambda$newThread$0(LoggingThreadFactory.java:124)
	at java.lang.Thread.run(Thread.java:750)





# importing the data to the new cluster will fix this issue

import data --member=server1 --region=/exampleRegion1 --dir=/tmp --parallel

gfsh>import data --member=server1 --region=/exampleRegion1 --dir=/tmp --parallel
Member  | Status | Message
------- | ------ | -----------------------------------------------------------------------------------
server1 | OK     | Data imported from file : /tmp on host : 172.16.204.135 to region : /exampleRegion1

gfsh>







[info 2025/10/17 14:28:34.718 UTC server1 <Function Execution Processor2> tid=0x48] Importing region exampleRegion1

[info 2025/10/17 14:28:34.724 UTC server1 <Function Execution Processor2> tid=0x48] Adding new type: PdxType[dsid=0, typenum=6994697
        name=Person
        fields=[
        id:String:0:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=-1
        name:String:1:1:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=1
        age:int:2:1:idx0(relativeOffset)=-4:idx1(vlfOffsetIndex)=-1]]

[info 2025/10/17 14:28:34.726 UTC server1 <Function Execution Processor2> tid=0x48] Importing type: PdxType[dsid=0, typenum=6994697
        name=Person
        fields=[
        id:String:0:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=-1
        name:String:1:1:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=1
        age:int:2:1:idx0(relativeOffset)=-4:idx1(vlfOffsetIndex)=-1]]

[info 2025/10/17 14:28:34.728 UTC server1 <Function Execution Processor2> tid=0x48] Adding new type: PdxType[dsid=0, typenum=12117126
        name=Student
        fields=[
        id:String:0:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=-1
        name:String:1:1:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=1
        age:int:2:1:idx0(relativeOffset)=-4:idx1(vlfOffsetIndex)=-1]]

[info 2025/10/17 14:28:34.730 UTC server1 <Function Execution Processor2> tid=0x48] Importing type: PdxType[dsid=0, typenum=12117126
        name=Student
        fields=[
        id:String:0:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=-1
        name:String:1:1:idx0(relativeOffset)=0:idx1(vlfOffsetIndex)=1
        age:int:2:1:idx0(relativeOffset)=-4:idx1(vlfOffsetIndex)=-1]]

[info 2025/10/17 14:28:34.733 UTC server1 <Function Execution Processor2> tid=0x48] Snapshot import of 2 entries (74 bytes) in region exampleRegion1 from file /tmp/exampleRegion1.gfd is complete








